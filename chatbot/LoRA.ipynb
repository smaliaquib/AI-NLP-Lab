{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1006f513-d0f4-455b-a5f6-a22b201ae70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"gpt2-small (124M)\" # Options: 'gpt2-small (124M)', 'gpt2-medium (355M)', 'gpt2-large (774M)', 'gpt2-xl (1558M)'\n",
    "weights = \"pretrained\" # Options: 'pretrained' or 'random'\n",
    "trainable_layers = \"last_block\" # Options: 'all', 'last_block', 'last_two_blocks', 'last_layer', 'lora', 'lora_alternative'\n",
    "trainable_token_pos = \"last\" # Options: 'first', 'last'\n",
    "average_embeddings = False\n",
    "context_length = \"longest_training_example\" # Options: 'longest_training_example', 'model_context_length' or integer value.\n",
    "lora_rank = 8 \n",
    "lora_alpha = 8\n",
    "no_padding = False\n",
    "num_epochs = 5\n",
    "batch_size = 8 \n",
    "accumulation_steps = 1\n",
    "disable_causal_mask = False\n",
    "ignore_index = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17f3e98-8e09-4e99-8d8f-16c7906b52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from utils import GPTModel, load_weights_into_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9863ed-5bab-4b02-b1bf-eec9636490cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "# This LoRA code is equivalent to LinearWithLoRA\n",
    "class LinearWithLoRAMerged(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
    "        return torch.nn.functional.linear(x, combined_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6bbdf4-bc90-4387-9423-05dd13cee195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=None, pad_token_id=50256, no_padding=False):\n",
    "        self.data = data\n",
    "        self.max_length = max_length if max_length is not None else self._longest_encoded_length(tokenizer)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text)[:self.max_length]\n",
    "            for text in self.data[\"text\"]\n",
    "        ]\n",
    "\n",
    "        if not no_padding:\n",
    "            # Pad sequences to the longest sequence\n",
    "            self.encoded_texts = [\n",
    "                et + [pad_token_id] * (self.max_length - len(et))\n",
    "                for et in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"sentiment\"]\n",
    "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self, tokenizer):\n",
    "        max_length = 0\n",
    "        for text in self.data[\"text\"]:\n",
    "            encoded_length = len(tokenizer.encode(text))\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b45cc1b-f4f8-4c57-894c-cfa31a319ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_model(choose_model, load_weights):\n",
    "\n",
    "    BASE_CONFIG = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"drop_rate\": 0.0,        # Dropout rate\n",
    "        \"qkv_bias\": True         # Query-key-value bias\n",
    "    }\n",
    "\n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    BASE_CONFIG.update(model_configs[choose_model])\n",
    "\n",
    "    if not load_weights:\n",
    "        torch.manual_seed(123)\n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "    if load_weights:\n",
    "        model_size = choose_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "        settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "        load_weights_into_gpt(model, params)\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e01892a-23d8-4362-b0a0-c31ea6aff625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device,\n",
    "                    trainable_token_pos=-1, ignore_index=-100, average_embeddings=False):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    model_output = model(input_batch)\n",
    "    if average_embeddings:\n",
    "        # Average over the sequence dimension (dim=1)\n",
    "        logits = model_output.mean(dim=1)\n",
    "    else:\n",
    "        # Select embeddings at the specified token position\n",
    "        logits = model_output[:, trainable_token_pos, :]\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch, ignore_index=ignore_index)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device,\n",
    "                     num_batches=None, trainable_token_pos=-1,\n",
    "                     ignore_index=-100, average_embeddings=False):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device,\n",
    "                trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "                average_embeddings=average_embeddings\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()  # Disable gradient tracking for efficiency\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None,\n",
    "                         trainable_token_pos=-1, average_embeddings=False):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            model_output = model(input_batch)\n",
    "            if average_embeddings:\n",
    "                # Average over the sequence dimension (dim=1)\n",
    "                logits = model_output.mean(dim=1)\n",
    "            else:\n",
    "                # Select embeddings at the specified token position\n",
    "                logits = model_output[:, trainable_token_pos, :]\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device,\n",
    "                   eval_iter, trainable_token_pos=-1,\n",
    "                   ignore_index=-100, average_embeddings=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter,\n",
    "            trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "            average_embeddings=average_embeddings\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter,\n",
    "            trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "            average_embeddings=average_embeddings\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa8e640a-951e-42a6-a1e3-9a0f8ee89f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device,\n",
    "                    trainable_token_pos=-1, ignore_index=-100, average_embeddings=False):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    model_output = model(input_batch)\n",
    "    if average_embeddings:\n",
    "        # Average over the sequence dimension (dim=1)\n",
    "        logits = model_output.mean(dim=1)\n",
    "    else:\n",
    "        # Select embeddings at the specified token position\n",
    "        logits = model_output[:, trainable_token_pos, :]\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch, ignore_index=ignore_index)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device,\n",
    "                     num_batches=None, trainable_token_pos=-1,\n",
    "                     ignore_index=-100, average_embeddings=False):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device,\n",
    "                trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "                average_embeddings=average_embeddings\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()  # Disable gradient tracking for efficiency\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None,\n",
    "                         trainable_token_pos=-1, average_embeddings=False):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            model_output = model(input_batch)\n",
    "            if average_embeddings:\n",
    "                # Average over the sequence dimension (dim=1)\n",
    "                logits = model_output.mean(dim=1)\n",
    "            else:\n",
    "                # Select embeddings at the specified token position\n",
    "                logits = model_output[:, trainable_token_pos, :]\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device,\n",
    "                   eval_iter, trainable_token_pos=-1,\n",
    "                   ignore_index=-100, average_embeddings=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter,\n",
    "            trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "            average_embeddings=average_embeddings\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter,\n",
    "            trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "            average_embeddings=average_embeddings\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter, max_steps=None, trainable_token_pos=-1,\n",
    "                            accumulation_steps=1, ignore_index=-100, average_embeddings=False):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for batch_idx, (input_batch, target_batch) in enumerate(train_loader):\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device,\n",
    "                trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "                average_embeddings=average_embeddings\n",
    "            )\n",
    "\n",
    "            # Use gradient accumulation if accumulation_steps > 1\n",
    "            # See https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html\n",
    "            # for an explanation\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "\n",
    "            # Use gradient accumulation if accumulation_steps > 1\n",
    "            is_update_step = ((batch_idx + 1) % accumulation_steps == 0) or ((batch_idx + 1) == len(train_loader))\n",
    "            if is_update_step:\n",
    "                optimizer.step()  # Update model weights using loss gradients\n",
    "                optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "\n",
    "            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter,\n",
    "                    trainable_token_pos=trainable_token_pos, ignore_index=ignore_index,\n",
    "                    average_embeddings=average_embeddings\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "            if max_steps is not None and global_step > max_steps:\n",
    "                break\n",
    "\n",
    "        # New: Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter,\n",
    "            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter,\n",
    "            trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings\n",
    "        )\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "        if max_steps is not None and global_step > max_steps:\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1d5cec-a187-49c0-81b2-b7808b25e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha, alternative=False):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Replace the Linear layer with LinearWithLoRA\n",
    "            if alternative:\n",
    "                setattr(model, name, LinearWithLoRAMerged(module, rank, alpha))\n",
    "            else:\n",
    "                setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # Recursively apply the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb199318-802e-490e-94d6-2d645017de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid --trainable_token_pos argument\n",
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "if trainable_token_pos == \"first\":\n",
    "    trainable_token_pos = 0\n",
    "elif trainable_token_pos == \"last\":\n",
    "    trainable_token_pos = -1\n",
    "else:\n",
    "    print(\"Invalid --trainable_token_pos argument\")\n",
    "\n",
    "###############################\n",
    "# Load model\n",
    "###############################\n",
    "\n",
    "if weights == \"pretrained\":\n",
    "    load_weights = True\n",
    "elif weights == \"random\":\n",
    "    load_weights = False\n",
    "else:\n",
    "    print(\"Invalid --weights argument.\")\n",
    "\n",
    "model = instantiate_model(model_size, load_weights)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if model_size == \"gpt2-small (124M)\":\n",
    "    in_features = 768\n",
    "elif model_size == \"gpt2-medium (355M)\":\n",
    "    in_features = 1024\n",
    "elif model_size == \"gpt2-large (774M)\":\n",
    "    in_features = 1280\n",
    "elif model_size == \"gpt2-xl (1558M)\":\n",
    "    in_features = 1600\n",
    "else:\n",
    "    print(\"Invalid --model_size argument\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model.out_head = torch.nn.Linear(in_features=in_features, out_features=2)\n",
    "\n",
    "if trainable_layers == \"last_layer\":\n",
    "    pass\n",
    "elif trainable_layers == \"last_block\" or trainable_layers == \"last_two_blocks\":\n",
    "    for param in model.trf_blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.final_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "    if trainable_layers == \"last_two_blocks\":\n",
    "        for param in model.trf_blocks[-2].parameters():\n",
    "            param.requires_grad = True\n",
    "elif trainable_layers == \"all\":\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "elif trainable_layers in (\"lora\", \"lora_alternative\"):\n",
    "    if trainable_layers == \"lora_alternative\":\n",
    "        alternative = True\n",
    "    else:\n",
    "        alternative = False\n",
    "    replace_linear_with_lora(model, rank=lora_rank, alpha=lora_alpha, alternative=alternative)\n",
    "else:\n",
    "    print(\"Invalid --trainable_layers argument.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9de26bda-c624-48f5-8c56-9dee19fba5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6589ba9-e7e4-43ce-8fbf-576adec536e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 21984, Test size: 5496\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"tweet-sentiment-extraction/train.csv\")\n",
    "dataset = dataset.dropna(subset=[\"text\", \"sentiment\"])\n",
    "\n",
    "dataset = dataset[[\"text\", \"sentiment\"]]\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "# Check the dataset sizes\n",
    "print(f\"Train size: {len(train_data)}, Test size: {len(test_data)}\")\n",
    "\n",
    "# Define the mapping\n",
    "sentiment_mapping = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
    "\n",
    "# Map sentiments in both train and test datasets\n",
    "train_data['sentiment'] = train_data['sentiment'].map(sentiment_mapping)\n",
    "test_data['sentiment'] = test_data['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaf5a595-b1e3-4c8c-8875-926bb567c14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SentimentDataset(\n",
    "    data=train_data,\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fade7404-fb6d-4b72-8ab8-3b7cd02838c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SentimentDataset(\n",
    "    data=test_data,\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96481f75-85ed-4b33-86d8-fafd6defbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01995663-cc8c-479d-9aff-09d8baeb32c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 100])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dddd2d82-69c2-4743-b604-c2a2b275c4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2748 training batches\n",
      "687 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5664d67-5754-4fde-a531-4c98995113f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " \"Dataset length 100 exceeds model's context length 1024. Reinitialize data sets with `max_length=1024`\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.max_length <= model.pos_emb.weight.shape[0], (\n",
    "        f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "        f\"length {model.pos_emb.weight.shape[0]}. Reinitialize data sets with \"\n",
    "        f\"`max_length={model.pos_emb.weight.shape[0]}`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026682f6-f1af-456e-b49c-65da12d89f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25c95457-239a-4d5d-9080-e884e88f0309",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m train_losses, val_losses, train_accs, val_accs, examples_seen \u001b[38;5;241m=\u001b[39m train_classifier_simple(\n\u001b[0;32m     10\u001b[0m     model, train_loader, test_loader, optimizer, device,\n\u001b[0;32m     11\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     12\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, trainable_token_pos\u001b[38;5;241m=\u001b[39mtrainable_token_pos,\n\u001b[0;32m     13\u001b[0m     accumulation_steps\u001b[38;5;241m=\u001b[39maccumulation_steps, average_embeddings\u001b[38;5;241m=\u001b[39maverage_embeddings\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m execution_time_minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "Cell \u001b[1;32mIn[25], line 114\u001b[0m, in \u001b[0;36mtrain_classifier_simple\u001b[1;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, max_steps, trainable_token_pos, accumulation_steps, ignore_index, average_embeddings)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Use gradient accumulation if accumulation_steps > 1\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# See https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# for an explanation\u001b[39;00m\n\u001b[0;32m    112\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m accumulation_steps\n\u001b[1;32m--> 114\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Use gradient accumulation if accumulation_steps > 1\u001b[39;00m\n\u001b[0;32m    117\u001b[0m is_update_step \u001b[38;5;241m=\u001b[39m ((batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ((batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    332\u001b[0m     (inputs,)\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[0;32m    337\u001b[0m )\n\u001b[0;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 220\u001b[0m             torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[0;32m    221\u001b[0m         )\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Train model\n",
    "###############################\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, test_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "    max_steps=None, trainable_token_pos=trainable_token_pos,\n",
    "    accumulation_steps=accumulation_steps, average_embeddings=average_embeddings\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n",
    "###############################\n",
    "# Evaluate model\n",
    "###############################\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device,\n",
    "    trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings\n",
    ")\n",
    "# val_accuracy = calc_accuracy_loader(\n",
    "#     val_loader, model, device,\n",
    "#     trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings\n",
    "# )\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device,\n",
    "    trainable_token_pos=trainable_token_pos, average_embeddings=average_embeddings\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "# print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791ce90-f88b-4bfc-9148-f7e9106e8ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23256c7-3dbd-4bb5-8914-5085d38e1702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
